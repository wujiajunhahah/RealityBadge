import SwiftUI
import AVFoundation
import CoreVideo
import UIKit

// ç”¨ä¸“ç”¨ UIView æ‰¿è½½ AVPreviewLayerï¼Œæ›´ç¨³
final class PreviewView: UIView {
    override class var layerClass: AnyClass { AVCaptureVideoPreviewLayer.self }
    var previewLayer: AVCaptureVideoPreviewLayer { layer as! AVCaptureVideoPreviewLayer }
}

struct CameraPreview: UIViewRepresentable {
    let session: AVCaptureSession
    func makeUIView(context: Context) -> PreviewView {
        let v = PreviewView()
        v.previewLayer.session = session
        v.previewLayer.videoGravity = .resizeAspectFill
        return v
    }
    func updateUIView(_ uiView: PreviewView, context: Context) {
        uiView.previewLayer.session = session
        uiView.previewLayer.videoGravity = .resizeAspectFill
    }
}

final class CameraController: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate, AVCapturePhotoCaptureDelegate, AVCaptureDepthDataOutputDelegate {
    @Published var isAuthorized = false
    @Published var isRunning = false
    @Published var progress: CGFloat = 0.0
    @Published var error: String?
    @Published var scores: SemanticScores = .init(objectConfidence: 0, handObjectIoU: 0, textImageSimilarity: 0)
    @Published var isVerified: Bool = false
    @Published var hint: String = "ä¿æŒç¨³å®šï¼Œå‡†å¤‡è‡ªåŠ¨æŠ“æ‹"
    @Published var bestLabel: String? = nil

    // æ–°å¢ï¼šæµæ°´æ¯è¯†åˆ«è®¡æ—¶å™¨
    @Published var cupDetectionTime: TimeInterval = 0
    @Published var isCupDetected: Bool = false
    @Published var autoCaptureEnabled: Bool = true

    private var cupDetectionStartTime: Date?
    private var autoCaptureTimer: Timer?

    let session = AVCaptureSession()
    private let queue = DispatchQueue(label: "rb.camera.queue")
    private let syncQueue = DispatchQueue(label: "rb.depth.queue")

    // æ–°å¢ï¼šè¯­ä¹‰å¼•æ“ä¸è®¾ç½®
    private let engine: SemanticEngine
    private let settings: RBSettings
    private let gate = StabilityGate(requiredFrames: 12, maxWindow: 18, driftTolerance: 0.12)
    private var lastPixelBuffer: CVPixelBuffer?
    private var lastDepthBuffer: CVPixelBuffer?

    init(engine: SemanticEngine, settings: RBSettings) {
        self.engine = engine
        self.settings = settings
        super.init()
        requestPermission()
    }

    private func requestPermission() {
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized:
            isAuthorized = true
            startSession()
        case .notDetermined:
            AVCaptureDevice.requestAccess(for: .video) { granted in
                DispatchQueue.main.async {
                    self.isAuthorized = granted
                    if granted { self.startSession() }
                }
            }
        default:
            isAuthorized = false
        }
    }

    private func startSession() {
        queue.async {
            self.configureSession()
            self.session.startRunning()
            DispatchQueue.main.async { self.isRunning = true }
        }
    }

    private func configureSession() {
        session.beginConfiguration()
        session.sessionPreset = .high

        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
            DispatchQueue.main.async { self.error = "æœªæ‰¾åˆ°åç½®ç›¸æœº" }
            session.commitConfiguration()
            return
        }
        guard let input = try? AVCaptureDeviceInput(device: device) else {
            DispatchQueue.main.async { self.error = "ç›¸æœºè¾“å…¥åˆ›å»ºå¤±è´¥" }
            session.commitConfiguration()
            return
        }
        if session.canAddInput(input) { session.addInput(input) }

        let output = AVCaptureVideoDataOutput()
        output.alwaysDiscardsLateVideoFrames = true
        output.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String:
                                kCVPixelFormatType_420YpCbCr8BiPlanarFullRange]
        output.setSampleBufferDelegate(self, queue: queue)
        if session.canAddOutput(output) { session.addOutput(output) }

        if let conn = output.connection(with: .video), conn.isVideoOrientationSupported {
            conn.videoOrientation = .portrait
        }

        // æ·±åº¦æ•°æ®ï¼ˆå¦‚æ”¯æŒï¼‰
        let depthOutput = AVCaptureDepthDataOutput()
        depthOutput.isFilteringEnabled = true
        if session.canAddOutput(depthOutput) {
            session.addOutput(depthOutput)
            depthOutput.setDelegate(self, callbackQueue: syncQueue)
            if let dconn = depthOutput.connection(with: .depthData), dconn.isVideoOrientationSupported { dconn.videoOrientation = .portrait }
            // é€‰æ‹©æ”¯æŒæ·±åº¦çš„ format
            if let best = device.activeFormat.supportedDepthDataFormats.first {
                try? device.lockForConfiguration(); device.activeDepthDataFormat = best; device.unlockForConfiguration()
            }
        }
        session.commitConfiguration()
    }

    func stop() {
        queue.async {
            if self.session.isRunning { self.session.stopRunning() }
            DispatchQueue.main.async { self.isRunning = false }
        }
    }

    // æ¢å¤ä¸‹ä¸€æ¬¡åˆ›ä½œï¼šæ¸…ç†çŠ¶æ€ã€é‡ç½®ç¨³å®šé—¨æ§›
    func resetCycle() {
        gate.reset()
        DispatchQueue.main.async {
            self.progress = 0
            self.isVerified = false
            self.scores = .init(objectConfidence: 0, handObjectIoU: 0, textImageSimilarity: 0)
            self.isCupDetected = false
            self.cupDetectionStartTime = nil
            self.cupDetectionTime = 0
            self.autoCaptureEnabled = true
            self.hint = "ä¿æŒç¨³å®šï¼Œå‡†å¤‡è‡ªåŠ¨æŠ“æ‹"
        }
    }

    // å°†å¸§é€å…¥è¯­ä¹‰å¼•æ“ï¼Œæ ¹æ®éªŒè¯æ¨¡å¼èåˆè¿›åº¦
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        if let pb = CMSampleBufferGetImageBuffer(sampleBuffer) {
            self.lastPixelBuffer = pb
        }
        let scores = engine.process(sampleBuffer: sampleBuffer)

        var fused: CGFloat
        switch settings.validationMode {
        case .strict:
            // å¿…é¡»æ‰‹-ç‰©äº’åŠ¨ + æœ‰ç‰©ä½“ + æœ‰è¯­ä¹‰ï¼šç”¨å‡ ä½•å¹³å‡ï¼Œæ˜¾å¾—æ›´"è‹›åˆ»"
            fused = pow(max(0.0001, scores.objectConfidence), 0.34)
                  * pow(max(0.0001, scores.handObjectIoU), 0.33)
                  * pow(max(0.0001, scores.textImageSimilarity), 0.33)
        case .standard:
            // ç‰©ä½“è¯†åˆ«ä¸ºä¸»ï¼Œè¯­ä¹‰è¾…åŠ©ï¼šé‡ç‰©ä½“ã€è½»è¯­ä¹‰
            fused = min(1.0, (scores.objectConfidence * 0.7 + scores.textImageSimilarity * 0.3))
        case .lenient:
            // ä»…è¯­ä¹‰åŒ¹é…ï¼šæ›´å®½æ¾
            fused = scores.textImageSimilarity
        }

        // è‹¥æ˜æ˜¾å‘½ä¸­ç›®æ ‡ä½†æ²¡æœ‰æ‰‹éƒ¨äº’åŠ¨ï¼ˆé€‚é…â€œç¬‘/æœˆäº®/é»‘è¡£æœâ€ç­‰æ— éœ€æ‰‹éƒ¨å‚ä¸çš„ç±»ç›®ï¼‰ï¼Œç»™äºˆä¿åº•èåˆ
        if scores.handObjectIoU < 0.15 && scores.objectConfidence >= 0.75 && scores.textImageSimilarity >= 0.6 {
            let fallback = min(1.0, scores.objectConfidence * 0.8 + scores.textImageSimilarity * 0.2)
            fused = max(fused, fallback)
        }

        // æŒç»­å‰è¿›ï¼ˆé¿å…æ¥å›æŠ–åŠ¨ï¼‰ï¼ŒåŒæ—¶é™åˆ¶åˆ° 0-1
        let newProgress = max(self.progress, min(1.0, fused * 1.05))

        // åŠ¨æ€é˜ˆå€¼ï¼ˆä¸åŒéªŒè¯æ¨¡å¼ï¼‰
        let threshold: CGFloat = {
            switch settings.validationMode {
            case .strict:   return 0.85
            case .standard: return 0.70
            case .lenient:  return 0.55
            }
        }()
        let result = gate.push(scores: scores, fused: fused, threshold: threshold, mode: settings.validationMode)

        // æ–°å¢ï¼šæµæ°´æ¯è¯†åˆ«é€»è¾‘
        let isCupDetected = self.isCupDetected(scores: scores)
        self.updateCupDetectionState(isCupDetected)

        DispatchQueue.main.async {
            self.progress = newProgress
            self.scores = scores
            self.isVerified = result.passed
            self.hint = result.hint
            self.bestLabel = self.engine.bestLabel()
        }
    }

    // æ·±åº¦æ•°æ®å›è°ƒ
    func depthDataOutput(_ output: AVCaptureDepthDataOutput, didOutput depthData: AVDepthData, timestamp: CMTime, connection: AVCaptureConnection) {
        lastDepthBuffer = depthData.depthDataMap
    }

    // æ–°å¢ï¼šæ£€æµ‹æ˜¯å¦è¯†åˆ«åˆ°æµæ°´æ¯
    private func isCupDetected(scores: SemanticScores) -> Bool {
        // æ£€æŸ¥æ˜¯å¦åŒ…å«æµæ°´æ¯ç›¸å…³çš„å…³é”®è¯åŒ¹é…
        // é™ä½é˜ˆå€¼ä»¥é€‚åº”VLMæ¨¡å‹çš„è¾“å‡ºèŒƒå›´
        let hasSemanticMatch = scores.textImageSimilarity > 0.3  // ä»0.7é™ä½åˆ°0.3
        let hasObjectMatch = scores.objectConfidence > 0.4      // ä»0.6é™ä½åˆ°0.4

        // åªè¦æœ‰è¯­ä¹‰åŒ¹é…æˆ–ç‰©ä½“åŒ¹é…å°±ç®—æ£€æµ‹åˆ°
        return hasSemanticMatch || hasObjectMatch
    }

    // æ–°å¢ï¼šæ›´æ–°æµæ°´æ¯æ£€æµ‹çŠ¶æ€
    private func updateCupDetectionState(_ detected: Bool) {
        DispatchQueue.main.async {
            if detected {
                if !self.isCupDetected {
                    // å¼€å§‹æ£€æµ‹åˆ°ç›®æ ‡
                    self.isCupDetected = true
                    self.cupDetectionStartTime = Date()
                    self.cupDetectionTime = 0
                    self.hint = "æ£€æµ‹åˆ°ç‰©ä½“ï¼Œæ­£åœ¨è®¡æ—¶..."
                } else {
                    // ç»§ç»­æ£€æµ‹ï¼Œæ›´æ–°æ—¶é—´
                    if let startTime = self.cupDetectionStartTime {
                        self.cupDetectionTime = Date().timeIntervalSince(startTime)

                        // æ£€æŸ¥æ˜¯å¦è¶…è¿‡2ç§’
                        if self.cupDetectionTime >= 2.0 && self.autoCaptureEnabled {
                            self.autoCaptureEnabled = false // é˜²æ­¢é‡å¤è§¦å‘
                            self.performAutoCapture()
                        }
                    }
                }
            } else {
                if self.isCupDetected {
                    // å¤±å»æ£€æµ‹
                    self.isCupDetected = false
                    self.cupDetectionStartTime = nil
                    self.cupDetectionTime = 0
                    self.hint = "ä¿æŒç¨³å®šï¼Œå‡†å¤‡è‡ªåŠ¨æŠ“æ‹"
                }
            }
        }
    }

    // æ–°å¢ï¼šæ‰§è¡Œè‡ªåŠ¨æ‹ç…§
    private func performAutoCapture() {
        DispatchQueue.main.async {
            self.hint = "ğŸ‰ è¯†åˆ«æˆåŠŸï¼æ­£åœ¨æ‹ç…§..."
            self.isVerified = true

            // æ·»åŠ æ‹ç…§é€»è¾‘
            self.capturePhotoFromStream()

            // é‡ç½®çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹æ¬¡æ£€æµ‹
            DispatchQueue.main.asyncAfter(deadline: .now() + 3.0) {
                self.isCupDetected = false
                self.cupDetectionStartTime = nil
                self.cupDetectionTime = 0
                self.autoCaptureEnabled = true
                self.isVerified = false
                self.hint = "ä¿æŒç¨³å®šï¼Œå‡†å¤‡è‡ªåŠ¨æŠ“æ‹"
            }
        }
    }

    // æ–°å¢ï¼šä»ç›¸æœºæµä¸­æ‹ç…§å¹¶ä¿å­˜
    private func capturePhotoFromStream() {
        // åˆ›å»ºç…§ç‰‡è¾“å‡º
        let photoOutput = AVCapturePhotoOutput()
        if session.canAddOutput(photoOutput) {
            session.addOutput(photoOutput)

            let settings = AVCapturePhotoSettings()
            settings.flashMode = .off

            photoOutput.capturePhoto(with: settings, delegate: self)
        }
    }

    // æ–°å¢ï¼šå¤„ç†æ‹ç…§ç»“æœ
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?) {
        guard let imageData = photo.fileDataRepresentation(),
              let uiImage = UIImage(data: imageData) else {
            DispatchQueue.main.async {
                self.hint = "æ‹ç…§å¤±è´¥ï¼Œè¯·é‡è¯•"
            }
            return
        }

        // ä¿å­˜ç…§ç‰‡åˆ°ç›¸å†Œ
        savePhotoToAlbum(uiImage)

        DispatchQueue.main.async {
            self.hint = "âœ… ç…§ç‰‡å·²ä¿å­˜åˆ°ç›¸å†Œï¼"
            RBHaptics.success()
        }
    }

    // å¿«ç…§ + å‰æ™¯æ©è†œï¼ˆå¯é€‰ï¼‰+ æ·±åº¦å›¾ï¼ˆå¯é€‰ï¼‰
    func snapshotWithMask(completion: @escaping (UIImage?, CGImage?, CGImage?) -> Void) {
        guard let pb = lastPixelBuffer else { completion(nil, nil, nil); return }
        var img: UIImage? = nil
        var mask: CGImage? = nil
        var depth: CGImage? = nil
        let group = DispatchGroup()
        group.enter()
        DispatchQueue.global(qos: .userInitiated).async {
            img = ForegroundMasker.image(from: pb)
            group.leave()
        }
        if #available(iOS 17.0, *) {
            group.enter()
            DispatchQueue.global(qos: .userInitiated).async {
                mask = ForegroundMasker.mask(from: pb)
                group.leave()
            }
        }
        if let db = lastDepthBuffer { // ç²—ç•¥è½¬æ¢ä¸º CGImage
            group.enter()
            DispatchQueue.global(qos: .userInitiated).async {
                let ci = CIImage(cvPixelBuffer: db)
                let ctx = CIContext()
                depth = ctx.createCGImage(ci, from: ci.extent)
                group.leave()
            }
        }
        group.notify(queue: .main) { completion(img, mask, depth) }
    }

    // æ–°å¢ï¼šä¿å­˜ç…§ç‰‡åˆ°ç›¸å†Œ
    private func savePhotoToAlbum(_ image: UIImage) {
        UIImageWriteToSavedPhotosAlbum(image, self, #selector(image(_:didFinishSavingWithError:contextInfo:)), nil)
    }

    // æ–°å¢ï¼šç…§ç‰‡ä¿å­˜å›è°ƒ
    @objc private func image(_ image: UIImage, didFinishSavingWithError error: Error?, contextInfo: UnsafeRawPointer) {
        if let error = error {
            DispatchQueue.main.async {
                self.hint = "ä¿å­˜å¤±è´¥ï¼š\(error.localizedDescription)"
            }
        } else {
            DispatchQueue.main.async {
                self.hint = "ğŸ“¸ ç…§ç‰‡å·²ä¿å­˜åˆ°ç›¸å†Œï¼"
            }
        }
    }
}

struct CaptureView: View {
    @EnvironmentObject var state: AppState
    @Environment(\.dismiss) private var dismiss
    @StateObject private var camera: CameraController
    @State private var isCapturing = false
    @State private var didPreHaptic = false

    init() {
        // ç›®æ ‡æ”¹ä¸ºï¼šç“¶å­/å±å¹•/ç”µè„‘ï¼ˆç§»é™¤ cupï¼Œåå‘ bottleï¼‰
        let engine = SemanticEngine(targetKeywords: [
            // ç“¶å­ / æ°´æ¯ï¼ˆè‹±æ–‡ä¼˜å…ˆï¼‰
            "bottle", "water bottle", "drinking bottle",
            // å±å¹•
            "screen", "monitor", "display", "television", "tv",
            // ç”µè„‘
            "computer", "laptop", "desktop", "notebook", "macbook", "pc",
            // ä¸­æ–‡å…³é”®è¯ï¼ˆä¾› VLM æ–‡æœ¬ç›¸ä¼¼åº¦ä½¿ç”¨ï¼‰
            "æ°´æ¯", "å±å¹•", "ç”µè„‘"
        ])
        // æ³¨æ„ï¼šè¿™é‡Œæ— æ³•ç›´æ¥è®¿é—® EnvironmentObject çš„ settingsï¼Œæ‰€ä»¥å…ˆåˆ›å»ºä¸€ä¸ªä¸´æ—¶ settingsã€‚
        // çœŸæ­£é¡¹ç›®é‡Œå»ºè®®æŠŠ settings ä»ä¸Šçº§æ³¨å…¥æˆ–æ”¹ä¸ºå•ä¾‹ã€‚
        let tmpSettings = RBSettings()
        _camera = StateObject(wrappedValue: CameraController(engine: engine, settings: tmpSettings))
    }

    var body: some View {
        ZStack {
            #if targetEnvironment(simulator)
            // æ¨¡æ‹Ÿå™¨æ²¡æœ‰ç›¸æœº
            LinearGradient(colors: [.black, .gray.opacity(0.6)], startPoint: .top, endPoint: .bottom)
                .overlay(
                    Text("ç›¸æœºåœ¨æ¨¡æ‹Ÿå™¨ä¸å¯ç”¨ï¼Œè¯·ç”¨çœŸæœºè¿è¡Œ")
                        .font(.system(.headline, design: .rounded))
                        .foregroundStyle(.white.opacity(0.9))
                        .padding()
                )
                .ignoresSafeArea()
            #else
            if camera.isAuthorized {
                CameraPreview(session: camera.session)
                    .ignoresSafeArea()
            } else {
                LinearGradient(colors: [.black, .gray.opacity(0.6)], startPoint: .top, endPoint: .bottom)
                    .ignoresSafeArea()
            }
            #endif

            VStack {
                HStack {
                    Button { dismiss() } label: {
                        Image(systemName: "chevron.backward")
                            .font(.system(size: 18, weight: .semibold))
                            .padding(10)
                            .background(.ultraThinMaterial, in: Circle())
                    }
                    Spacer()
                }
                .padding(.horizontal, 16)
                .padding(.top, 12)

                Spacer()

                // çŠ¶æ€ + è°ƒè¯•ä¿¡æ¯ï¼ˆå¯å…³ï¼‰
                VStack(spacing: 6) {
                    Text(camera.isAuthorized ? "è¯­ä¹‰å¿«é—¨å·²å°±ç»ª" : "éœ€è¦ç›¸æœºæƒé™")
                        .font(.system(.headline, design: .rounded))
                        .foregroundStyle(.white.opacity(0.9))

                    // æ–°å¢ï¼šæµæ°´æ¯æ£€æµ‹çŠ¶æ€
                    if camera.isCupDetected {
                        HStack(spacing: 8) {
                            Image(systemName: "viewfinder")
                                .foregroundColor(.green)
                            Text("ç›®æ ‡æ£€æµ‹ä¸­...")
                                .font(.system(.headline, design: .rounded))
                                .foregroundStyle(.green.opacity(0.9))
                            Text(String(format: "%.1fs", camera.cupDetectionTime))
                                .font(.system(.subheadline, design: .rounded))
                                .foregroundStyle(.green.opacity(0.7))
                        }
                        .padding(.horizontal, 12)
                        .padding(.vertical, 6)
                        .background(.ultraThinMaterial, in: RoundedRectangle(cornerRadius: 20))
                    }

                    Text("æ¨¡å¼ï¼š\(RBSettings().validationMode.rawValue)  Â·  è¿›åº¦ï¼š\(Int(camera.progress * 100))%")
                        .font(.system(.caption, design: .rounded))
                        .foregroundStyle(.white.opacity(0.7))
                    calibrationBars(scores: camera.scores)
                    Text(camera.hint)
                        .font(.system(.caption2, design: .rounded))
                        .foregroundStyle(.white.opacity(0.85))
                }
                .padding(.bottom, 8)

                // è¿›åº¦ç¯
                ZStack {
                    Circle()
                        .strokeBorder(Color.white.opacity(0.25), lineWidth: 6)
                        .frame(width: 120, height: 120)
                    Circle()
                        .trim(from: 0, to: camera.progress)
                        .stroke(Color.white, style: StrokeStyle(lineWidth: 6, lineCap: .round))
                        .rotationEffect(.degrees(-90))
                        .frame(width: 120, height: 120)
                }
                .padding(.bottom, 30)
            }
        }
        .onChange(of: camera.progress) { old, v in
            if !didPreHaptic, v >= 0.92 {
                didPreHaptic = true
                RBHaptics.light()
            }
        }
        .onReceive(NotificationCenter.default.publisher(for: .rbCaptureResume)) { _ in
            // é¢„è§ˆ/ä¿å­˜æµç¨‹ç»“æŸï¼Œæ¢å¤ä¸‹ä¸€æ¬¡åˆ›ä½œ
            didPreHaptic = false
            isCapturing = false
            camera.resetCycle()
        }
        .onChange(of: camera.isVerified) { _, ok in if ok { triggerCapture() } }
        .onDisappear { camera.stop() }
    }



    private func triggerCapture() {
        guard !isCapturing else { return }
        isCapturing = true
        RBHaptics.success()
        camera.snapshotWithMask { img, mask, depth in
            guard let img else {
                // å›é€€åˆ°è€çš„å¾½ç« é¢„è§ˆ
                let new = Badge(title: self.displayNameForLabel(camera.bestLabel), date: .now, style: state.settings.style, done: true, symbol: "seal")
                state.recentBadges.insert(new, at: 0)
                BadgeStore.save(state.recentBadges)
                state.sheet = .badgePreview(new)
                return
            }
            state.sheet = .capturePreview(image: img, mask: mask, title: self.displayNameForLabel(camera.bestLabel), depth: depth)
        }
    }

    private func displayNameForLabel(_ label: String?) -> String {
        guard let l = label?.lowercased() else { return "è¯­ä¹‰å¿«é—¨" }
        if ["screen","monitor","display","television","tv"].contains(where: { l.contains($0) }) { return "å±å¹•" }
        if ["computer","laptop","desktop","notebook","macbook","pc"].contains(where: { l.contains($0) }) { return "ç”µè„‘" }
        if ["bottle","water bottle","drinking bottle"].contains(where: { l.contains($0) }) { return "æ°´æ¯" }
        return l
    }

    @ViewBuilder
    private func calibrationBars(scores: SemanticScores) -> some View {
        VStack(spacing: 6) {
            HStack(spacing: 8) {
                bar(title: "ç‰©ä½“", value: scores.objectConfidence, color: .green)
                bar(title: "äº’åŠ¨", value: scores.handObjectIoU, color: .yellow)
                bar(title: "è¯­ä¹‰", value: scores.textImageSimilarity, color: .orange)
            }
            .frame(height: 6)
            .padding(.horizontal, 24)
        }
    }

    private func bar(title: String, value: CGFloat, color: Color) -> some View {
        GeometryReader { geo in
            ZStack(alignment: .leading) {
                Capsule().fill(.white.opacity(0.18))
                Capsule().fill(color).frame(width: max(0, min(geo.size.width, geo.size.width * value)))
            }
            .accessibilityLabel(Text("\(title) \(Int(value * 100))%"))
        }
    }

    // ä¸å†æä¾›å–æ™¯ç°æ¡†ï¼ˆè¯­ä¹‰å¿«é—¨ï¼‰
}
